{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing\n",
    "\n",
    "In this script, we undertake preprocessing of all standardized Netflow datasets (version 2), to render it compatible for analysis by Large Language Models. All datasets will be stored locally under the efficient streaming *arrow* format and published to Hugging Face as private datasets.\n",
    "\n",
    "The resulting datasets share a uniform structure with the following features:\n",
    "\n",
    "| Feature Name | Description|\n",
    "|------------------------------|-----------------------------------------------|\n",
    "|*input*                | A tabular netflow entry encoded as text using key-value pairs separated by commas to represent the feature name and value pairs. For instance, a network flow originally represented as a row within a CSV table is transformed into text as follows: ```IPV4_SRC_ADDR: 149.171.126.0 [...] TCP_FLAGS: 25, FLOW_DURATION_MILLISECONDS: 15\"```\n",
    "| *output*                | Label associated the with the network flows, 0 being benign and 1 malicious|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "load_dotenv()\n",
    "HUGGING_FACE_WRITE_TOKEN = getenv(\"HUGGING_FACE_WRITE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_dataset_to_hub(dataset, dataset_name):\n",
    "    dataset.push_to_hub(f\"Jetlime/{dataset_name}\", token=HUGGING_FACE_WRITE_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NF-UNSW-NB15-v2\n",
    "\n",
    "Building a training and testing set with a 95-5% ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"NF-UNSW-NB15-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening ../data_raw/NF-UNSW-NB15-v2.csv\n",
      "Merging all columns in parralel.\n",
      "[########################################] | 100% Completed | 289.39 s\n",
      "[########################################] | 100% Completed | 289.47 s\n",
      "Stringifying the column: 100%|█| 2390275/2390275 [00:03<00:00, 765589.79 example\n",
      "Casting to class labels: 100%|█| 2390275/2390275 [00:04<00:00, 589446.88 example\n",
      "Casting to class labels: 100%|█| 2390275/2390275 [00:04<00:00, 516273.48 example\n",
      "Saving the dataset (5/5 shards): 100%|█| 2270761/2270761 [00:08<00:00, 281144.90\n",
      "Saving the dataset (1/1 shards): 100%|█| 119514/119514 [00:00<00:00, 284217.31 e\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python3 -W ignore ./dataset_gen_helper.py {DATASET_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2493a62eeab64420ae5546243df0fa1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9273c60617df4d479c08146e5103c6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/455 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971efa267bd347748b3ba018266a0b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/455 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56295d4e5c474d72a6946fb786b5a108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/455 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77dc98d7562a4fa6bd0463dc2a6c09d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/455 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738aa300d2ff4f2ebe891fc0c0ab0f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/455 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef148ab93a4244c1899aa1db2fcd6b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65b39d6f4574c92be5a65a8be87e962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df19aa6c726847aea99ce0d716a8cff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_from_disk(f\"./{DATASET_NAME}\", keep_in_memory=True)\n",
    "push_dataset_to_hub(dataset, DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NF-CSE-CIC-IDS2018-v2\n",
    "\n",
    "Building a training and testing set with a 95-5% ratio.\n",
    "Building solely a testing set made of 5% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"NF-CSE-CIC-IDS2018-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening ../data_raw/NF-CSE-CIC-IDS2018-v2.csv\n",
      "Merging all columns in parralel.\n",
      "[                                        ] | 0% Completed | 462.30 ss"
     ]
    }
   ],
   "source": [
    "!python3 -W ignore ./dataset_gen_helper.py {DATASET_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory ./NF-CSE-CIC-IDS2018-v2 not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDATASET_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m push_dataset_to_hub(dataset, DATASET_NAME)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/load.py:2689\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2687\u001b[0m fs, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m url_to_fs(dataset_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[1;32m   2688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(dataset_path):\n\u001b[0;32m-> 2689\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_INFO_FILENAME)) \u001b[38;5;129;01mand\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(\n\u001b[1;32m   2691\u001b[0m     posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_STATE_JSON_FILENAME)\n\u001b[1;32m   2692\u001b[0m ):\n\u001b[1;32m   2693\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory ./NF-CSE-CIC-IDS2018-v2 not found"
     ]
    }
   ],
   "source": [
    "dataset = load_from_disk(f\"./{DATASET_NAME}\", keep_in_memory=True)\n",
    "push_dataset_to_hub(dataset, DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NF-UQ-NIDS-v2\n",
    "\n",
    "\n",
    "Building solely a testing set made of 5% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"NF-UQ-NIDS-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening ./data_raw/NF-UQ-NIDS-v2.csv\n",
      "(<dask_expr.expr.Scalar: expr=ReadCSV(d4d2cec).size() // 46, dtype=int64>, 46)\n",
      "(<dask_expr.expr.Scalar: expr=(SplitTake(frame=Split(frame=ReadCSV(d4d2cec), frac=[0.050000000000000044, 0.95], random_state=1608637542, shuffle=False), i=0, ndim=2)).size() // 46, dtype=int64>, 46)\n",
      "Merging all columns in parralel.\n",
      "[########################################] | 100% Completed | 452.50 s\n",
      "Stringifying the column: 100%|█| 3798653/3798653 [00:04<00:00, 865015.58 example\n",
      "Casting to class labels: 100%|█| 3798653/3798653 [00:05<00:00, 655034.99 example\n",
      "Casting to class labels: 100%|█| 3798653/3798653 [00:06<00:00, 566633.90 example\n",
      "Saving the dataset (8/8 shards): 100%|█| 3798653/3798653 [00:02<00:00, 1496135.8\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python3 -W ignore ./dataset_gen_helper.py {DATASET_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642e93fb7dca429ca0b449af30eb24e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e552f9d94a948dc8a861a826287236a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/475 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ee41437ab6447eae3be85c514c6b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/475 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53c872f53b045fb926388568ec8268e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/475 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8215b4895804ccb975d1479023f3af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/475 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c465ee683be045a38fc524bddf9fb9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/475 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ddb880072747dba7d51db500d5748b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/475 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ae4db25012485b9ef6bc035d620b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/475 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1b59f2e576499b8bd3e8c48be16b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/475 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_from_disk(f\"./{DATASET_NAME}\", keep_in_memory=True)\n",
    "push_dataset_to_hub(dataset, DATASET_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
