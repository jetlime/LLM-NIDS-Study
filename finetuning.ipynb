{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Finetuning\n",
    "\n",
    "In this script, we investigate the usage of finetuning on the *UNSW-NB15* dataset using various preprocessing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key-Value pairs Text Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "Dataset.cleanup_cache_files\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "HUGGING_FACE_READ_TOKEN = getenv(\"HUGGING_FACE_READ_TOKEN\")\n",
    "\n",
    "dataset = load_dataset(\"arrow\", data_dir=\"NF-UNSW-NB15/\", streaming=True,split=\"train\")\n",
    "\n",
    "classes = dataset.features[\"chosen\"].names\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/967304590420382862', creation_time=1715755222120, experiment_id='967304590420382862', last_update_time=1715755222120, lifecycle_stage='active', name='Testing finetuning', tags={}>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "import torch\n",
    "import mlflow\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n",
    "\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "mlflow.set_experiment(experiment_name=\"Testing finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "new_model = \"OrpoLlama-3-8B\"\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, token=HUGGING_FACE_READ_TOKEN)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    attn_implementation=attn_implementation,\n",
    "    token=HUGGING_FACE_READ_TOKEN\n",
    ")\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_template(row):\n",
    "    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
    "    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    format_chat_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 4.84k/4.84k [00:00<00:00, 8.40MB/s]\n",
      "Downloading data: 100%|██████████| 71.5M/71.5M [00:38<00:00, 1.83MB/s]\n",
      "Downloading data: 100%|██████████| 7.95M/7.95M [00:05<00:00, 1.40MB/s]\n",
      "Generating train split: 100%|██████████| 1460806/1460806 [00:00<00:00, 2688909.34 examples/s]\n",
      "Generating test split: 100%|██████████| 162312/162312 [00:00<00:00, 2763535.32 examples/s]\n",
      "Map (num_proc=16): 100%|██████████| 1460806/1460806 [00:05<00:00, 244189.26 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConcatenationTable\n",
       "prompt: string\n",
       "chosen: string\n",
       "rejected: string\n",
       "----\n",
       "prompt: [[\"### Question:IPV4_SRC_ADDR: 59.166.0.3, L4_SRC_PORT: 58855, IPV4_DST_ADDR: 149.171.126.6, L4_DST_PORT: 80, PROTOCOL: 6, L7_PROTO: 7.0, IN_BYTES: 1684, OUT_BYTES: 10168, IN_PKTS: 14, OUT_PKTS: 18, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 1948 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.6, L4_SRC_PORT: 39483, IPV4_DST_ADDR: 149.171.126.7, L4_DST_PORT: 6496, PROTOCOL: 6, L7_PROTO: 0.0, IN_BYTES: 5590, OUT_BYTES: 92028, IN_PKTS: 98, OUT_PKTS: 96, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 0 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.5, L4_SRC_PORT: 40896, IPV4_DST_ADDR: 149.171.126.8, L4_DST_PORT: 111, PROTOCOL: 17, L7_PROTO: 11.0, IN_BYTES: 568, OUT_BYTES: 320, IN_PKTS: 4, OUT_PKTS: 4, TCP_FLAGS: 0, FLOW_DURATION_MILLISECONDS: 258 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.1, L4_SRC_PORT: 10258, IPV4_DST_ADDR: 149.171.126.9, L4_DST_PORT: 111, PROTOCOL: 17, L7_PROTO: 0.0, IN_BYTES: 568, OUT_BYTES: 312, IN_PKTS: 4, OUT_PKTS: 4, TCP_FLAGS: 0, FLOW_DURATION_MILLISECONDS: 0 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 175.45.176.0, L4_SRC_PORT: 54749, IPV4_DST_ADDR: 149.171.126.13, L4_DST_PORT: 8089, PROTOCOL: 6, L7_PROTO: 0.0, IN_BYTES: 160, OUT_BYTES: 80, IN_PKTS: 4, OUT_PKTS: 2, TCP_FLAGS: 18, FLOW_DURATION_MILLISECONDS: 2836 ### Answer:\",...,\"### Question:IPV4_SRC_ADDR: 59.166.0.1, L4_SRC_PORT: 61781, IPV4_DST_ADDR: 149.171.126.7, L4_DST_PORT: 80, PROTOCOL: 6, L7_PROTO: 7.0, IN_BYTES: 1684, OUT_BYTES: 10168, IN_PKTS: 14, OUT_PKTS: 18, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 1082 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.6, L4_SRC_PORT: 20958, IPV4_DST_ADDR: 149.171.126.4, L4_DST_PORT: 22, PROTOCOL: 6, L7_PROTO: 92.0, IN_BYTES: 3728, OUT_BYTES: 5474, IN_PKTS: 32, OUT_PKTS: 24, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 7 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.3, L4_SRC_PORT: 52886, IPV4_DST_ADDR: 149.171.126.1, L4_DST_PORT: 5190, PROTOCOL: 6, L7_PROTO: 0.0, IN_BYTES: 1064, OUT_BYTES: 2260, IN_PKTS: 12, OUT_PKTS: 12, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 4 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.9, L4_SRC_PORT: 56535, IPV4_DST_ADDR: 149.171.126.5, L4_DST_PORT: 30906, PROTOCOL: 6, L7_PROTO: 0.0, IN_BYTES: 320, OUT_BYTES: 1816, IN_PKTS: 6, OUT_PKTS: 8, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 33 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 175.45.176.1, L4_SRC_PORT: 7934, IPV4_DST_ADDR: 149.171.126.14, L4_DST_PORT: 111, PROTOCOL: 6, L7_PROTO: 11.0, IN_BYTES: 552, OUT_BYTES: 336, IN_PKTS: 10, OUT_PKTS: 8, TCP_FLAGS: 19, FLOW_DURATION_MILLISECONDS: 821 ### Answer:\"],[\"### Question:IPV4_SRC_ADDR: 59.166.0.5, L4_SRC_PORT: 63503, IPV4_DST_ADDR: 149.171.126.3, L4_DST_PORT: 53, PROTOCOL: 17, L7_PROTO: 0.0, IN_BYTES: 130, OUT_BYTES: 162, IN_PKTS: 2, OUT_PKTS: 2, TCP_FLAGS: 0, FLOW_DURATION_MILLISECONDS: 0 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.3, L4_SRC_PORT: 42175, IPV4_DST_ADDR: 149.171.126.8, L4_DST_PORT: 80, PROTOCOL: 6, L7_PROTO: 7.0, IN_BYTES: 1684, OUT_BYTES: 10168, IN_PKTS: 14, OUT_PKTS: 18, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 1005 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.7, L4_SRC_PORT: 53106, IPV4_DST_ADDR: 149.171.126.9, L4_DST_PORT: 53872, PROTOCOL: 6, L7_PROTO: 0.0, IN_BYTES: 4392, OUT_BYTES: 3080, IN_PKTS: 28, OUT_PKTS: 30, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 0 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.0, L4_SRC_PORT: 5951, IPV4_DST_ADDR: 149.171.126.0, L4_DST_PORT: 28409, PROTOCOL: 6, L7_PROTO: 36.0, IN_BYTES: 2958, OUT_BYTES: 32374, IN_PKTS: 48, OUT_PKTS: 50, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 15 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.1, L4_SRC_PORT: 39838, IPV4_DST_ADDR: 149.171.126.5, L4_DST_PORT: 21, PROTOCOL: 6, L7_PROTO: 1.0, IN_BYTES: 2934, OUT_BYTES: 3742, IN_PKTS: 52, OUT_PKTS: 54, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 2171 ### Answer:\",...,\"### Question:IPV4_SRC_ADDR: 59.166.0.0, L4_SRC_PORT: 2653, IPV4_DST_ADDR: 149.171.126.3, L4_DST_PORT: 19786, PROTOCOL: 6, L7_PROTO: 0.0, IN_BYTES: 2646, OUT_BYTES: 23838, IN_PKTS: 42, OUT_PKTS: 44, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 0 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 10.40.182.1, L4_SRC_PORT: 0, IPV4_DST_ADDR: 224.0.0.5, L4_DST_PORT: 0, PROTOCOL: 89, L7_PROTO: 85.0, IN_BYTES: 768, OUT_BYTES: 0, IN_PKTS: 12, OUT_PKTS: 0, TCP_FLAGS: 0, FLOW_DURATION_MILLISECONDS: 110009 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.2, L4_SRC_PORT: 64873, IPV4_DST_ADDR: 149.171.126.6, L4_DST_PORT: 22, PROTOCOL: 6, L7_PROTO: 92.0, IN_BYTES: 3728, OUT_BYTES: 5474, IN_PKTS: 32, OUT_PKTS: 24, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 1130 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.7, L4_SRC_PORT: 54711, IPV4_DST_ADDR: 149.171.126.1, L4_DST_PORT: 80, PROTOCOL: 6, L7_PROTO: 7.0, IN_BYTES: 1038, OUT_BYTES: 820, IN_PKTS: 8, OUT_PKTS: 10, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 3 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.8, L4_SRC_PORT: 53775, IPV4_DST_ADDR: 149.171.126.5, L4_DST_PORT: 53, PROTOCOL: 17, L7_PROTO: 5.0, IN_BYTES: 146, OUT_BYTES: 178, IN_PKTS: 2, OUT_PKTS: 2, TCP_FLAGS: 0, FLOW_DURATION_MILLISECONDS: 1 ### Answer:\"],...,[\"### Question:IPV4_SRC_ADDR: 59.166.0.5, L4_SRC_PORT: 25737, IPV4_DST_ADDR: 149.171.126.9, L4_DST_PORT: 80, PROTOCOL: 6, L7_PROTO: 7.0, IN_BYTES: 18890, OUT_BYTES: 1087890, IN_PKTS: 354, OUT_PKTS: 746, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 17851 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.8, L4_SRC_PORT: 60643, IPV4_DST_ADDR: 149.171.126.7, L4_DST_PORT: 17648, PROTOCOL: 6, L7_PROTO: 0.0, IN_BYTES: 424, OUT_BYTES: 8824, IN_PKTS: 8, OUT_PKTS: 12, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 0 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.8, L4_SRC_PORT: 16336, IPV4_DST_ADDR: 149.171.126.5, L4_DST_PORT: 54867, PROTOCOL: 6, L7_PROTO: 0.0, IN_BYTES: 424, OUT_BYTES: 8824, IN_PKTS: 8, OUT_PKTS: 12, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 273 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.2, L4_SRC_PORT: 22846, IPV4_DST_ADDR: 149.171.126.0, L4_DST_PORT: 53, PROTOCOL: 17, L7_PROTO: 0.0, IN_BYTES: 146, OUT_BYTES: 178, IN_PKTS: 2, OUT_PKTS: 2, TCP_FLAGS: 0, FLOW_DURATION_MILLISECONDS: 0 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.2, L4_SRC_PORT: 61716, IPV4_DST_ADDR: 149.171.126.7, L4_DST_PORT: 44281, PROTOCOL: 6, L7_PROTO: 36.0, IN_BYTES: 4222, OUT_BYTES: 64070, IN_PKTS: 72, OUT_PKTS: 74, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 32 ### Answer:\",...,\"### Question:IPV4_SRC_ADDR: 59.166.0.6, L4_SRC_PORT: 26329, IPV4_DST_ADDR: 149.171.126.1, L4_DST_PORT: 25, PROTOCOL: 6, L7_PROTO: 3.0, IN_BYTES: 37274, OUT_BYTES: 3276, IN_PKTS: 52, OUT_PKTS: 40, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 45 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.9, L4_SRC_PORT: 62534, IPV4_DST_ADDR: 149.171.126.6, L4_DST_PORT: 36304, PROTOCOL: 6, L7_PROTO: 36.0, IN_BYTES: 2750, OUT_BYTES: 26742, IN_PKTS: 44, OUT_PKTS: 46, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 13 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.9, L4_SRC_PORT: 1879, IPV4_DST_ADDR: 149.171.126.2, L4_DST_PORT: 15802, PROTOCOL: 17, L7_PROTO: 0.0, IN_BYTES: 536, OUT_BYTES: 304, IN_PKTS: 4, OUT_PKTS: 4, TCP_FLAGS: 0, FLOW_DURATION_MILLISECONDS: 0 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.0, L4_SRC_PORT: 2548, IPV4_DST_ADDR: 149.171.126.5, L4_DST_PORT: 53, PROTOCOL: 17, L7_PROTO: 5.0, IN_BYTES: 130, OUT_BYTES: 162, IN_PKTS: 2, OUT_PKTS: 2, TCP_FLAGS: 0, FLOW_DURATION_MILLISECONDS: 1 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.6, L4_SRC_PORT: 58490, IPV4_DST_ADDR: 149.171.126.1, L4_DST_PORT: 21, PROTOCOL: 6, L7_PROTO: 1.0, IN_BYTES: 2934, OUT_BYTES: 3742, IN_PKTS: 52, OUT_PKTS: 54, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 847 ### Answer:\"],[\"### Question:IPV4_SRC_ADDR: 59.166.0.7, L4_SRC_PORT: 48034, IPV4_DST_ADDR: 149.171.126.4, L4_DST_PORT: 6881, PROTOCOL: 6, L7_PROTO: 0.0, IN_BYTES: 1540, OUT_BYTES: 1644, IN_PKTS: 16, OUT_PKTS: 18, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 0 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 175.45.176.0, L4_SRC_PORT: 42344, IPV4_DST_ADDR: 149.171.126.11, L4_DST_PORT: 21, PROTOCOL: 6, L7_PROTO: 1.0, IN_BYTES: 2006, OUT_BYTES: 810, IN_PKTS: 14, OUT_PKTS: 14, TCP_FLAGS: 19, FLOW_DURATION_MILLISECONDS: 696 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.6, L4_SRC_PORT: 60154, IPV4_DST_ADDR: 149.171.126.9, L4_DST_PORT: 1626, PROTOCOL: 6, L7_PROTO: 36.0, IN_BYTES: 4862, OUT_BYTES: 80850, IN_PKTS: 84, OUT_PKTS: 86, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 59 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.8, L4_SRC_PORT: 56205, IPV4_DST_ADDR: 149.171.126.5, L4_DST_PORT: 61180, PROTOCOL: 6, L7_PROTO: 11.0, IN_BYTES: 3968, OUT_BYTES: 2456, IN_PKTS: 18, OUT_PKTS: 18, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 4 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.5, L4_SRC_PORT: 6228, IPV4_DST_ADDR: 149.171.126.1, L4_DST_PORT: 4333, PROTOCOL: 6, L7_PROTO: 36.0, IN_BYTES: 2542, OUT_BYTES: 21294, IN_PKTS: 40, OUT_PKTS: 42, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 11 ### Answer:\",...,\"### Question:IPV4_SRC_ADDR: 59.166.0.4, L4_SRC_PORT: 32301, IPV4_DST_ADDR: 149.171.126.3, L4_DST_PORT: 53, PROTOCOL: 17, L7_PROTO: 5.0, IN_BYTES: 146, OUT_BYTES: 178, IN_PKTS: 2, OUT_PKTS: 2, TCP_FLAGS: 0, FLOW_DURATION_MILLISECONDS: 0 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.1, L4_SRC_PORT: 40792, IPV4_DST_ADDR: 149.171.126.6, L4_DST_PORT: 143, PROTOCOL: 6, L7_PROTO: 4.0, IN_BYTES: 7808, OUT_BYTES: 14996, IN_PKTS: 122, OUT_PKTS: 126, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 679 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.5, L4_SRC_PORT: 59570, IPV4_DST_ADDR: 149.171.126.8, L4_DST_PORT: 5190, PROTOCOL: 6, L7_PROTO: 0.0, IN_BYTES: 1920, OUT_BYTES: 4312, IN_PKTS: 22, OUT_PKTS: 24, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 5 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.8, L4_SRC_PORT: 24823, IPV4_DST_ADDR: 149.171.126.9, L4_DST_PORT: 25, PROTOCOL: 6, L7_PROTO: 3.0, IN_BYTES: 37390, OUT_BYTES: 3380, IN_PKTS: 52, OUT_PKTS: 42, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 25 ### Answer:\",\"### Question:IPV4_SRC_ADDR: 59.166.0.8, L4_SRC_PORT: 44154, IPV4_DST_ADDR: 149.171.126.9, L4_DST_PORT: 5190, PROTOCOL: 6, L7_PROTO: 0.0, IN_BYTES: 1244, OUT_BYTES: 2574, IN_PKTS: 16, OUT_PKTS: 18, TCP_FLAGS: 27, FLOW_DURATION_MILLISECONDS: 0 ### Answer:\"]]\n",
       "chosen: [[\"0\",\"0\",\"0\",\"0\",\"0\",...,\"0\",\"0\",\"0\",\"0\",\"1\"],[\"0\",\"0\",\"0\",\"0\",\"0\",...,\"0\",\"0\",\"0\",\"0\",\"0\"],...,[\"0\",\"0\",\"0\",\"0\",\"0\",...,\"0\",\"0\",\"0\",\"0\",\"0\"],[\"0\",\"1\",\"0\",\"0\",\"0\",...,\"0\",\"0\",\"0\",\"0\",\"0\"]]\n",
       "rejected: [[\"1\",\"1\",\"1\",\"1\",\"1\",...,\"1\",\"1\",\"1\",\"1\",\"0\"],[\"1\",\"1\",\"1\",\"1\",\"1\",...,\"1\",\"1\",\"1\",\"1\",\"1\"],...,[\"1\",\"1\",\"1\",\"1\",\"1\",...,\"1\",\"1\",\"1\",\"1\",\"1\"],[\"1\",\"0\",\"1\",\"1\",\"1\",...,\"1\",\"1\",\"1\",\"1\",\"1\"]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"Jetlime/NF-UNSW-NB15\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "# dataset = dataset.shuffle(seed=42).select(range(1000))\n",
    "\n",
    "def format_chat_template(row):\n",
    "    # row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
    "    # row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
    "    # row[\"chosen\"] = str(row[\"chosen\"])\n",
    "    # row[\"rejected\"] = str(row[\"rejected\"])\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc= os.cpu_count(),\n",
    ")\n",
    "\n",
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/.local/lib/python3.10/site-packages/trl/trainer/orpo_trainer.py:247: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1460806/1460806 [18:02<00:00, 1349.78 examples/s]\n",
      "/home/paul/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6501' max='182600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  6501/182600 10:55:59 < 296:14:59, 0.17 it/s, Epoch 0.04/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 781001f4-f65a-44a1-b88d-386434feadb4)') - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "orpo_args = ORPOConfig(\n",
    "    learning_rate=8e-6,\n",
    "    beta=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    max_length=1024,\n",
    "    max_prompt_length=512,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    report_to=\"mlflow\",\n",
    "    output_dir=\"results\"\n",
    ")\n",
    "\n",
    "trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=orpo_args,\n",
    "    train_dataset=dataset,\n",
    "    # eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush memory\n",
    "del trainer, model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reload tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "\n",
    "# Merge adapter with base model\n",
    "model = PeftModel.from_pretrained(model, new_model)\n",
    "model = model.merge_and_unload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
