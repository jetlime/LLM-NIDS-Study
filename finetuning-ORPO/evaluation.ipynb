{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: ['input', 'output', 'Attack'],\n",
       "    n_shards: 1\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "Dataset.cleanup_cache_files\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "HUGGING_FACE_READ_TOKEN = getenv(\"HUGGING_FACE_READ_TOKEN\")\n",
    "\n",
    "dataset = load_dataset(\"Jetlime/NF-UNSW-NB15-v2\", streaming=True, split=\"test\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = dataset.features[\"output\"].names\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7df117e48364847b2cedcd8e22537f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# We choose the instruction version of Llama 3 as the foundational\n",
    "# model showed difficulties to answer in the required format.\n",
    "# This is an expected behavior as these models were not trained to\n",
    "# understand instructions but simply to predict the sequence of words.\n",
    "model_id = \"OrpoLlama-3-8B\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cuda\",\n",
    "    token=HUGGING_FACE_READ_TOKEN,\n",
    "    pad_token_id = 50256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classification_pipeline(netflow):\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"instruction\", \"content\": \"You are a cybersecurity expert tasked with classifying network flows as either malicious or benign. If you determine the network flow is benign, respond with '0'. If you determine the network flow is malicious, respond with '1'. For example, if given the following network flow: 'IPV4_SRC_ADDR: 59.166.0.7, L4_SRC_PORT: 53030, IPV4_DST_ADDR: 149.171.126.7, L4_DST_PORT: 44287, PROTOCOL: 6, L7_PROTO: 0.0, IN_YTES: 8928, IN_TS: 14, OUT_TES: 320, OUT_S: 6, TCP_AGS: 27, CLIENT_CP_AGS: 27, SERVER_CP_LAGS: 19, FLOW_URATION_ILLISECONDS: 0, DURATION_N: 0, DURATION_UT: 0, MIN_L: 31, MAX_L: 32, LONGEST_OW_T: 1500, SHORTEST_OW_: 52, MIN__PKT_N: 52, MAX__T_: 1500, SRC__T_ECOND_YTES: 8928.0, DST_O_C_COND_TES: 320.0, RETRANSMITTED__TES: 4252, RETRANSMITTED__TS: 3, RETRANSMITTED_T_TES: 0, RETRANSMITTED_T_TS: 0, SRC__DST_VG_ROUGHPUT: 71424000, DST__RC_G_ROUGHPUT: 2560000, NUM_TS___28_TES: 14, NUM_TS_8__6_YTES: 0, NUM_KTS_6__2_TES: 0, NUM_TS_2__24_TES: 0, NUM_TS_24__14_TES: 6, TCP_WIN_MAX_N: 5792, TCP_N_X_T: 10136, ICMP_PE: 39936, ICMP_PV4_TYPE: 156, DNS_QUERY_ID: 0, DNS_QUERY_TYPE: 0, DNS_TTL_ANSWER: 0, FTP_COMMAND_RET_CODE: 0.0' and you assess it as benign, you would respond with '0'. If you assess it as malicious, you would respond with '1'. You are only allowed to respond with '0' or '1'. If requested, provide an explanation for your classification, detailing the reasoning and which feature values influenced your decision.\"},\n",
    "        {\"role\": \"input\", \"content\": netflow},\n",
    "    ]\n",
    "\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=100,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.01,\n",
    "        top_p=0.9,\n",
    "        \n",
    "    )\n",
    "\n",
    "    return outputs[0][\"generated_text\"][len(prompt):]\n",
    "\n",
    "classification_pipeline(\"IPV4_SRC_ADDR: 149.171.126.0, L4_SRC_PORT: 62073, IPV4_DST_ADDR: 59.166.0.5, L4_DST_PORT: 56082, PROTOCOL: 6, L7_PROTO: 0.0, IN_BYTES: 9672, OUT_BYTES: 416, IN_PKTS: 11, OUT_PKTS: 8, TCP_FLAGS: 25, FLOW_DURATION_MILLISECONDS: 15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:03<00:28,  3.14it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.23it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from numpy import mean, array\n",
    "from scipy.stats import sem\n",
    "\n",
    "prediction_labels = []\n",
    "inference_times = []\n",
    "\n",
    "for i in tqdm(dataset, total=100):\n",
    "    start_time = time.time()  # Record the start time\n",
    "    prediction = classification_pipeline(i['input'])\n",
    "    end_time = time.time()  # Record the end time\n",
    "\n",
    "    prediction_labels.append(prediction)\n",
    "    \n",
    "    inference_time = end_time - start_time  # Calculate the inference time\n",
    "    inference_times.append(inference_time)\n",
    "\n",
    "# Convert inference_times to a numpy array for statistical operations\n",
    "inference_times = array(inference_times)\n",
    "\n",
    "# Compute the mean inference time\n",
    "mean_inference_time = mean(inference_times)\n",
    "\n",
    "# Compute the standard error of the mean (SEM)\n",
    "standard_error = sem(inference_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2967293405532837"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004772357772076936"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "standard_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 100/120000 [00:01<22:46, 87.76it/s] \n"
     ]
    }
   ],
   "source": [
    "true_labels = []\n",
    "for i in tqdm(dataset, total=120000):\n",
    "    true_labels.append(i[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9688    0.3229    0.4844        96\n",
      "   malicious     0.0441    0.7500    0.0833         4\n",
      "\n",
      "    accuracy                         0.3400       100\n",
      "   macro avg     0.5064    0.5365    0.2839       100\n",
      "weighted avg     0.9318    0.3400    0.4683       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "prediction_labels = [int(item) for item in prediction_labels]\n",
    "\n",
    "target_names = ['benign', 'malicious']\n",
    "\n",
    "print(classification_report(true_labels, prediction_labels, digits=4, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix_indices(predictions, true_labels):\n",
    "    TN_indices = []\n",
    "    FN_indices = []\n",
    "    FP_indices = []\n",
    "    TP_indices = []\n",
    "    \n",
    "    for i, (pred, true) in enumerate(zip(predictions, true_labels)):\n",
    "        if pred == 0 and true == 0:\n",
    "            TN_indices.append(i)\n",
    "        elif pred == 0 and true == 1:\n",
    "            FN_indices.append(i)\n",
    "        elif pred == 1 and true == 0:\n",
    "            FP_indices.append(i)\n",
    "        elif pred == 1 and true == 1:\n",
    "            TP_indices.append(i)\n",
    "    \n",
    "    return TN_indices, FN_indices, FP_indices, TP_indices\n",
    "\n",
    "TN_indices, FN_indices, FP_indices, TP_indices = compute_confusion_matrix_indices(prediction_labels, true_labels)\n",
    "\n",
    "print(\"TN indices:\", TN_indices)\n",
    "print(\"FN indices:\", FN_indices)\n",
    "print(\"FP indices:\", FP_indices)\n",
    "print(\"TP indices:\", TP_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
