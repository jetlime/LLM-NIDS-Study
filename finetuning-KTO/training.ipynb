{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Finetuning\n",
    "\n",
    "In this script, we investigate the usage of finetuning on the *UNSW-NB15* dataset using various preprocessing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key-Value pairs Text Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the latest cached version of the dataset since Jetlime/NF-UNSW-NB15-v2 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/paul/.cache/huggingface/datasets/Jetlime___nf-unsw-nb15-v2/default/0.0.0/e787691e196b078564cfc32297f511298a45a15f (last modified on Wed May 22 12:23:27 2024).\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "Dataset.cleanup_cache_files\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "HUGGING_FACE_READ_TOKEN = getenv(\"HUGGING_FACE_READ_TOKEN\")\n",
    "\n",
    "dataset = load_dataset(\"Jetlime/NF-UNSW-NB15-v2\", streaming=False, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/967304590420382862', creation_time=1715755222120, experiment_id='967304590420382862', last_update_time=1715755222120, lifecycle_stage='active', name='Testing finetuning', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "import torch\n",
    "import mlflow\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from trl import KTOConfig, KTOTrainer, setup_chat_format\n",
    "\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "mlflow.set_experiment(experiment_name=\"Testing finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "new_model = \"OrpoLlama-3-8B\"\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, token=HUGGING_FACE_READ_TOKEN)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    attn_implementation=attn_implementation,\n",
    "    token=HUGGING_FACE_READ_TOKEN\n",
    ")\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'Attack'],\n",
       "    num_rows: 113538\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use only a small subset of the training set for a first finetuning trial\n",
    "dataset = dataset.train_test_split(test_size=0.95, seed=123, stratify_by_column=\"Attack\")\n",
    "dataset_finetuning = dataset[\"train\"]\n",
    "dataset_finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=16): 100%|██████████| 113538/113538 [00:01<00:00, 100567.04 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'Attack', 'prompt', 'label', 'completion'],\n",
       "    num_rows: 113538\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def format_chat_template(row):\n",
    "    row['prompt'] = row[\"input\"]\n",
    "    if random.randrange(0,1):\n",
    "        row[\"label\"] = False\n",
    "        if row[\"output\"] == 1:\n",
    "            row[\"completion\"] = '0'\n",
    "        else:\n",
    "            row[\"completion\"] = '1'\n",
    "    else:\n",
    "        row[\"label\"] = True\n",
    "        row[\"completion\"] = str(row[\"output\"])\n",
    "    return row\n",
    "\n",
    "dataset_finetuning = dataset_finetuning.map(\n",
    "    format_chat_template, num_proc=os.cpu_count()\n",
    ")\n",
    "dataset_finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/.local/lib/python3.10/site-packages/trl/trainer/kto_trainer.py:417: UserWarning: When using DPODataCollatorWithPadding, you should set `max_length` in the KTOTrainer's init it will be set to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/trl/trainer/kto_trainer.py:427: UserWarning: When using DPODataCollatorWithPadding, you should set `max_prompt_length` in the KTOTrainer's init it will be set to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/home/paul/.local/lib/python3.10/site-packages/trl/trainer/kto_trainer.py:457: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your KTOConfig we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Tokenizing train dataset: 100%|██████████| 113538/113538 [01:26<00:00, 1313.22 examples/s]\n",
      "Extracting KL train dataset: 100%|██████████| 113538/113538 [00:08<00:00, 14077.15 examples/s]\n",
      "Processing tokenized train dataset: 100%|██████████| 113538/113538 [00:42<00:00, 2678.53 examples/s]\n",
      "Processing tokenized train KL dataset: 100%|██████████| 113538/113538 [00:40<00:00, 2805.04 examples/s]\n",
      "Filtering desirable examples: 100%|██████████| 113538/113538 [01:37<00:00, 1163.86 examples/s]\n",
      "Filtering undesirable examples: 100%|██████████| 113538/113538 [01:36<00:00, 1170.86 examples/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m KTOConfig(\n\u001b[1;32m      2\u001b[0m     beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m      3\u001b[0m     desirable_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m      4\u001b[0m     undesirable_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m      5\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results-KTO/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m kto_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mKTOTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_finetuning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m kto_trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     16\u001b[0m kto_trainer\u001b[38;5;241m.\u001b[39msave_model(new_model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/trl/trainer/kto_trainer.py:599\u001b[0m, in \u001b[0;36mKTOTrainer.__init__\u001b[0;34m(self, model, ref_model, args, train_dataset, eval_dataset, tokenizer, data_collator, model_init, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, compute_metrics, model_adapter_name, ref_adapter_name)\u001b[0m\n\u001b[1;32m    597\u001b[0m des_weight_lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m((\u001b[38;5;28mlen\u001b[39m(undesirable) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mundesirable_weight \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(desirable)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    598\u001b[0m des_weight_upper_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m((\u001b[38;5;28mlen\u001b[39m(undesirable) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mundesirable_weight \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(desirable)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.33\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 599\u001b[0m und_weight_lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m((\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdesirable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesirable_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mundesirable\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1.33\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    600\u001b[0m und_weight_upper_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m((\u001b[38;5;28mlen\u001b[39m(desirable) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesirable_weight \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(undesirable)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    602\u001b[0m des_weight_in_range \u001b[38;5;241m=\u001b[39m des_weight_lower_bound \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesirable_weight \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m des_weight_upper_bound\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "training_args = KTOConfig(\n",
    "    beta=0.1,\n",
    "    desirable_weight=1.0,\n",
    "    undesirable_weight=1.0,\n",
    "    output_dir=\"./results-KTO/\"\n",
    ")\n",
    "\n",
    "kto_trainer = KTOTrainer(\n",
    "    model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_finetuning,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "kto_trainer.train()\n",
    "kto_trainer.save_model(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush memory\n",
    "del trainer, model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reload tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "\n",
    "# Merge adapter with base model\n",
    "model = PeftModel.from_pretrained(model, new_model)\n",
    "model = model.merge_and_unload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
