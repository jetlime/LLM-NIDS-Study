{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing\n",
    "\n",
    "In this script, we undertake preprocessing of all standardized Netflow datasets (version 2), to render it compatible for analysis by Large Language Models. All datasets will be stored locally under the efficient streaming *arrow* format and published to Hugging Face as private datasets.\n",
    "\n",
    "The resulting datasets share a uniform structure with the following features:\n",
    "\n",
    "| Feature Name                     | Description                                   |\n",
    "|------------------------------|-----------------------------------------------|\n",
    "|*input*                | A tabular netflow entry encoded as text using key-value pairs separated by commas to represent the feature name and value pairs. For instance, a network flow originally represented as a row within a CSV table is transformed into text as follows: ```IPV4_SRC_ADDR: 149.171.126.0 [...] TCP_FLAGS: 25, FLOW_DURATION_MILLISECONDS: 15\"```\n",
    "| *output*                | Label associated the with the network flows, 0 being benign and 1 malicious|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "load_dotenv()\n",
    "HUGGING_FACE_WRITE_TOKEN = getenv(\"HUGGING_FACE_WRITE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function for all datasets\n",
    "def encode_dataset(dataset_name):\n",
    "    df = pd.read_csv(f\"./data_raw/{dataset_name}.csv\")\n",
    "\n",
    "    # Merge all column entries as one single string using key-value pairs\n",
    "    def create_key_value_pairs_str(row):\n",
    "        return ', '.join([f\"{column}: {row[column]}\" for column in df.columns.drop(\"output\")])\n",
    "\n",
    "    # Obtain the output label which shall be predicted by the LLM\n",
    "    df.rename(columns={'Label': 'output'}, inplace=True)\n",
    "\n",
    "    # Remove the prediction labels from the data which will be encoded\n",
    "    del df[\"Attack\"]\n",
    "\n",
    "    # Merge all remaining columns\n",
    "    df['input'] = df.apply(create_key_value_pairs_str, axis=1)\n",
    "\n",
    "    df = df[['input', 'output']]\n",
    "\n",
    "    return df\n",
    "\n",
    "def save_to_arrow_disk(df, dataset_name):\n",
    "    examples = df.to_dict()\n",
    "\n",
    "    prompt_template_qa = \"\"\"{input}\"\"\"\n",
    "\n",
    "    num_examples = len(examples[\"input\"])\n",
    "    finetuning_dataset_input_output = {}\n",
    "    finetuning_dataset_input_output['input'] = []\n",
    "    finetuning_dataset_input_output['output'] = []\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        input = examples[\"input\"][i]\n",
    "        output = examples[\"output\"][i]\n",
    "\n",
    "        text_with_prompt_template_qa = prompt_template_qa.format(input=input)\n",
    "        finetuning_dataset_input_output['input'].append(text_with_prompt_template_qa)\n",
    "        finetuning_dataset_input_output['output'].append(output)\n",
    "\n",
    "    finetuning_dataset = datasets.Dataset.from_dict(finetuning_dataset_input_output)\n",
    "    finetuning_dataset = finetuning_dataset.class_encode_column(\"output\")\n",
    "    finetuning_dataset = finetuning_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123, stratify_by_column=\"output\")\n",
    "    finetuning_dataset.save_to_disk(f\"./{dataset_name}/\")\n",
    "    return finetuning_dataset\n",
    "\n",
    "def push_dataset_to_hub(dataset, dataset_name):\n",
    "    dataset.push_to_hub(f\"Jetlime/{dataset_name}\", private=True, token=HUGGING_FACE_WRITE_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NF-UNSW-NB15-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IPV4_SRC_ADDR: 59.166.0.5, L4_SRC_PORT: 1305, IPV4_DST_ADDR: 149.171.126.8, L4_DST_PORT: 21, PROTOCOL: 6, L7_PROTO: 1.0, IN_BYTES: 9, IN_PKTS: 1, OUT_BYTES: 193, OUT_PKTS: 3, TCP_FLAGS: 24, CLIENT_TCP_FLAGS: 24, SERVER_TCP_FLAGS: 16, FLOW_DURATION_MILLISECONDS: 0, DURATION_IN: 0, DURATION_OUT: 0, MIN_TTL: 31, MAX_TTL: 32, LONGEST_FLOW_PKT: 89, SHORTEST_FLOW_PKT: 52, MIN_IP_PKT_LEN: 52, MAX_IP_PKT_LEN: 89, SRC_TO_DST_SECOND_BYTES: 456.0, DST_TO_SRC_SECOND_BYTES: 435.0, RETRANSMITTED_IN_BYTES:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 input  \\\n",
       "0  IPV4_SRC_ADDR: 59.166.0.5, L4_SRC_PORT: 1305, IPV4_DST_ADDR: 149.171.126.8, L4_DST_PORT: 21, PROTOCOL: 6, L7_PROTO: 1.0, IN_BYTES: 9, IN_PKTS: 1, OUT_BYTES: 193, OUT_PKTS: 3, TCP_FLAGS: 24, CLIENT_TCP_FLAGS: 24, SERVER_TCP_FLAGS: 16, FLOW_DURATION_MILLISECONDS: 0, DURATION_IN: 0, DURATION_OUT: 0, MIN_TTL: 31, MAX_TTL: 32, LONGEST_FLOW_PKT: 89, SHORTEST_FLOW_PKT: 52, MIN_IP_PKT_LEN: 52, MAX_IP_PKT_LEN: 89, SRC_TO_DST_SECOND_BYTES: 456.0, DST_TO_SRC_SECOND_BYTES: 435.0, RETRANSMITTED_IN_BYTES:...   \n",
       "\n",
       "   output  \n",
       "0       0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = encode_dataset(\"NF-UNSW-NB15-v2\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stringifying the column: 100%|██████████| 2390275/2390275 [00:02<00:00, 807168.72 examples/s] \n",
      "Casting to class labels: 100%|██████████| 2390275/2390275 [00:03<00:00, 623607.65 examples/s]\n",
      "Saving the dataset (5/5 shards): 100%|██████████| 2151247/2151247 [00:09<00:00, 224425.26 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 239028/239028 [00:01<00:00, 223493.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = save_to_arrow_disk(df, \"NF-UNSW-NB15-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 431/431 [00:03<00:00, 116.53ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 431/431 [00:03<00:00, 114.42ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 431/431 [00:03<00:00, 115.81ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 431/431 [00:03<00:00, 114.06ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 431/431 [00:03<00:00, 116.20ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 5/5 [00:22<00:00,  4.57s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 240/240 [00:02<00:00, 117.13ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.56s/it]\n"
     ]
    }
   ],
   "source": [
    "push_dataset_to_hub(dataset, \"NF-UNSW-NB15-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NF-ToN-IoT-v2 - Ignored for Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IPV4_SRC_ADDR: 192.168.1.193, L4_SRC_PORT: 49235, IPV4_DST_ADDR: 192.168.1.33, L4_DST_PORT: 4444, PROTOCOL: 6, L7_PROTO: 0.0, IN_BYTES: 155392, IN_PKTS: 202, OUT_BYTES: 34552, OUT_PKTS: 149, TCP_FLAGS: 24, CLIENT_TCP_FLAGS: 24, SERVER_TCP_FLAGS: 24, FLOW_DURATION_MILLISECONDS: 4294952, DURATION_IN: 15, DURATION_OUT: 15, MIN_TTL: 128, MAX_TTL: 128, LONGEST_FLOW_PKT: 1500, SHORTEST_FLOW_PKT: 40, MIN_IP_PKT_LEN: 40, MAX_IP_PKT_LEN: 1500, SRC_TO_DST_SECOND_BYTES: 155392.0, DST_TO_SRC_SECOND_BYTE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 input  \\\n",
       "0  IPV4_SRC_ADDR: 192.168.1.193, L4_SRC_PORT: 49235, IPV4_DST_ADDR: 192.168.1.33, L4_DST_PORT: 4444, PROTOCOL: 6, L7_PROTO: 0.0, IN_BYTES: 155392, IN_PKTS: 202, OUT_BYTES: 34552, OUT_PKTS: 149, TCP_FLAGS: 24, CLIENT_TCP_FLAGS: 24, SERVER_TCP_FLAGS: 24, FLOW_DURATION_MILLISECONDS: 4294952, DURATION_IN: 15, DURATION_OUT: 15, MIN_TTL: 128, MAX_TTL: 128, LONGEST_FLOW_PKT: 1500, SHORTEST_FLOW_PKT: 40, MIN_IP_PKT_LEN: 40, MAX_IP_PKT_LEN: 1500, SRC_TO_DST_SECOND_BYTES: 155392.0, DST_TO_SRC_SECOND_BYTE...   \n",
       "\n",
       "   output  \n",
       "0       1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = encode_dataset(\"NF-ToN-IoT-v2\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_to_arrow_disk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43msave_to_arrow_disk\u001b[49m(df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNF-ToN-IoT-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save_to_arrow_disk' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = save_to_arrow_disk(df, \"NF-ToN-IoT-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "push_dataset_to_hub(dataset, \"NF-ToN-IoT-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NF-BoT-IoT-v2 - Ignored for Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = encode_dataset(\"NF-BoT-IoT-v2\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = save_to_arrow_disk(df, \"NF-BoT-IoT-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "push_dataset_to_hub(dataset, \"NF-BoT-IoT-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NF-CSE-CIC-IDS2018-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = encode_dataset(\"NF-CSE-CIC-IDS2018-v2\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = save_to_arrow_disk(df, \"NF-CSE-CIC-IDS2018-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "push_dataset_to_hub(dataset, \"NF-CSE-CIC-IDS2018-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NF-UQ-NIDS-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = encode_dataset(\"NF-UQ-NIDS-v2\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "push_dataset_to_hub(dataset, \"NF-UQ-NIDS-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "push_dataset_to_hub(dataset, \"NF-UQ-NIDS-v2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
